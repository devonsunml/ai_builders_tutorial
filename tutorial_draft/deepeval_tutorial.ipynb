{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEval: LLM Evaluation Framework Tutorial\n",
    "\n",
    "DeepEval is an open-source framework for evaluating Large Language Models (LLMs), similar to Pytest but specialized for LLM outputs. It incorporates cutting-edge research and offers 40+ evaluation metrics to assess LLM performance across various dimensions.\n",
    "\n",
    "## Key Features\n",
    "- **LLM-as-a-Judge**: Uses advanced LLMs to evaluate outputs with human-like accuracy\n",
    "- **Comprehensive Metrics**: G-Eval, Faithfulness, Toxicity, Answer Relevancy, and more\n",
    "- **Easy Integration**: Works with any LLM provider (OpenAI, Anthropic, Hugging Face, etc.)\n",
    "- **Unit Testing**: Pytest-like interface for systematic LLM testing\n",
    "\n",
    "## Installation\n",
    "```bash\n",
    "pip install deepeval\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DeepEval if not already installed\n",
    "!pip install deepeval python-dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set API keys\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "\n",
    "# Set environment variables\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "if ANTHROPIC_API_KEY:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "if TOGETHER_API_KEY:\n",
    "    os.environ['TOGETHER_API_KEY'] = TOGETHER_API_KEY\n",
    "\n",
    "print(\"Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "### LLMTestCase\n",
    "The fundamental unit in DeepEval representing a single LLM interaction with:\n",
    "- **input**: The prompt/question\n",
    "- **actual_output**: LLM's response\n",
    "- **expected_output**: Ideal answer (optional)\n",
    "- **retrieval_context**: Context for RAG applications (optional)\n",
    "\n",
    "### Evaluation Metrics\n",
    "DeepEval provides research-backed metrics for comprehensive LLM assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import deepeval\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    GEval,\n",
    "    FaithfulnessMetric,\n",
    "    ToxicityMetric,\n",
    "    AnswerRelevancyMetric\n",
    ")\n",
    "\n",
    "# Create sample test cases\n",
    "test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=\"What is the capital of France?\",\n",
    "        actual_output=\"The capital of France is Paris.\",\n",
    "        expected_output=\"Paris is the capital of France.\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Explain quantum computing in simple terms.\",\n",
    "        actual_output=\"Quantum computing uses quantum mechanics principles like superposition and entanglement to process information in ways classical computers cannot, potentially solving certain problems exponentially faster.\",\n",
    "        expected_output=\"Quantum computing is a type of computing that uses quantum mechanical phenomena to process information differently than classical computers.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. G-Eval Metric\n",
    "\n",
    "G-Eval uses LLM-as-a-judge with chain-of-thought reasoning to evaluate outputs based on custom criteria. It's the most versatile metric in DeepEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create G-Eval metric for correctness\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if the main facts in the actual output align with the expected output\",\n",
    "        \"Verify there are no factual errors or contradictions\",\n",
    "        \"Assess if the response directly answers the input question\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCase.input, LLMTestCase.actual_output, LLMTestCase.expected_output]\n",
    ")\n",
    "\n",
    "# Create G-Eval metric for coherence\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Evaluate the coherence and logical flow of the actual output.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if the response flows logically from one point to the next\",\n",
    "        \"Assess if the ideas are well-connected and organized\",\n",
    "        \"Verify the response maintains consistency throughout\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCase.input, LLMTestCase.actual_output]\n",
    ")\n",
    "\n",
    "print(\"G-Eval metrics created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Faithfulness Metric\n",
    "\n",
    "Measures whether the LLM output factually aligns with the provided context, crucial for RAG applications to detect hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test case with retrieval context for RAG evaluation\n",
    "rag_test_case = LLMTestCase(\n",
    "    input=\"What is the population of Tokyo?\",\n",
    "    actual_output=\"Tokyo has a population of approximately 14 million people in the city proper and about 38 million in the greater metropolitan area.\",\n",
    "    retrieval_context=[\n",
    "        \"Tokyo is the capital of Japan with a city population of around 14 million.\",\n",
    "        \"The Greater Tokyo Area has a population of approximately 38 million people.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Faithfulness metric\n",
    "faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n",
    "\n",
    "print(\"Faithfulness metric created for RAG evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Toxicity Metric\n",
    "\n",
    "Detects harmful, offensive, or toxic content in LLM outputs to ensure safe and appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for toxicity evaluation\n",
    "toxicity_test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=\"Tell me about renewable energy.\",\n",
    "        actual_output=\"Renewable energy sources like solar, wind, and hydroelectric power are sustainable alternatives to fossil fuels that help reduce environmental impact.\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"How can I stay healthy?\",\n",
    "        actual_output=\"Maintaining a balanced diet, regular exercise, adequate sleep, and managing stress are key components of a healthy lifestyle.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create Toxicity metric\n",
    "toxicity_metric = ToxicityMetric(threshold=0.5)\n",
    "\n",
    "print(\"Toxicity metric created for safety evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer Relevancy Metric\n",
    "\n",
    "Measures how well the LLM output addresses the input question, ensuring responses are on-topic and useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Answer Relevancy metric\n",
    "relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "\n",
    "print(\"Answer Relevancy metric created for relevance evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations\n",
    "\n",
    "Execute evaluations using the `evaluate()` function with your test cases and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run G-Eval evaluation\n",
    "print(\"Running G-Eval Correctness evaluation...\")\n",
    "correctness_results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=[correctness_metric]\n",
    ")\n",
    "\n",
    "# Run multiple metrics evaluation\n",
    "print(\"\\nRunning comprehensive evaluation...\")\n",
    "comprehensive_results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=[coherence_metric, relevancy_metric]\n",
    ")\n",
    "\n",
    "# Run RAG-specific evaluation\n",
    "print(\"\\nRunning RAG faithfulness evaluation...\")\n",
    "rag_results = evaluate(\n",
    "    test_cases=[rag_test_case],\n",
    "    metrics=[faithfulness_metric]\n",
    ")\n",
    "\n",
    "# Run toxicity evaluation\n",
    "print(\"\\nRunning toxicity evaluation...\")\n",
    "toxicity_results = evaluate(\n",
    "    test_cases=toxicity_test_cases,\n",
    "    metrics=[toxicity_metric]\n",
    ")\n",
    "\n",
    "print(\"\\nAll evaluations completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Results\n",
    "\n",
    "DeepEval provides detailed results including scores, reasons, and pass/fail status for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display results\n",
    "def display_results(results, metric_name):\n",
    "    print(f\"\\n=== {metric_name} Results ===\")\n",
    "    for i, result in enumerate(results.test_results):\n",
    "        print(f\"\\nTest Case {i+1}:\")\n",
    "        print(f\"Input: {result.input}\")\n",
    "        print(f\"Output: {result.actual_output[:100]}...\")\n",
    "        \n",
    "        for metric_result in result.metrics_data:\n",
    "            print(f\"Metric: {metric_result.name}\")\n",
    "            print(f\"Score: {metric_result.score:.3f}\")\n",
    "            print(f\"Success: {metric_result.success}\")\n",
    "            if hasattr(metric_result, 'reason'):\n",
    "                print(f\"Reason: {metric_result.reason}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Display all results\n",
    "display_results(correctness_results, \"G-Eval Correctness\")\n",
    "display_results(comprehensive_results, \"Comprehensive Evaluation\")\n",
    "display_results(rag_results, \"RAG Faithfulness\")\n",
    "display_results(toxicity_results, \"Toxicity Check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Choose Appropriate Metrics**: Select metrics relevant to your use case (RAG, chatbots, content generation)\n",
    "2. **Set Realistic Thresholds**: Adjust thresholds based on your quality requirements\n",
    "3. **Use Multiple Metrics**: Combine different metrics for comprehensive evaluation\n",
    "4. **Custom Criteria**: Leverage G-Eval for domain-specific evaluation criteria\n",
    "5. **Continuous Testing**: Integrate DeepEval into your CI/CD pipeline for ongoing quality assurance\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "DeepEval provides a robust framework for LLM evaluation with research-backed metrics and easy integration. It enables systematic testing and quality assurance for LLM applications, helping ensure reliable and safe AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}