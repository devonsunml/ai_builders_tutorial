{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq API Complete Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Groq is a revolutionary AI inference platform powered by **Language Processing Units (LPUs)** that deliver lightning-fast inference speeds. Unlike traditional GPUs, Groq's LPU architecture is specifically designed for sequential computations in language models, achieving **up to 750 tokens/second** - dramatically faster than conventional solutions.\n",
    "\n",
    "### Key Features:\n",
    "- **Ultra-fast inference**: 10x faster than traditional GPU-based solutions\n",
    "- **OpenAI-compatible API**: Easy migration from existing projects\n",
    "- **Advanced tool use**: Function calling and web search capabilities\n",
    "- **Streaming support**: Real-time response generation\n",
    "- **Multiple model support**: Llama 3.3, Mixtral, and more\n",
    "\n",
    "### Why Groq Matters:\n",
    "Speed transforms user experience in AI applications. Real-time chatbots, instant code generation, and responsive AI assistants become possible with Groq's sub-second response times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install groq python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq, AsyncGroq\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Set your API key (replace with your actual key)\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize client\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "Start with simple text generation using Groq's fastest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_chat_completion(prompt: str) -> str:\n",
    "    \"\"\"Generate a basic chat completion\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",  # Fast, high-quality model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "result = basic_chat_completion(\"Explain quantum computing in simple terms\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Stream responses for real-time user experience, crucial for interactive applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_chat(prompt: str):\n",
    "    \"\"\"Stream chat responses in real-time\"\"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    print(\"AI Response: \", end=\"\")\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()  # New line at end\n",
    "\n",
    "# Example usage\n",
    "streaming_chat(\"Write a haiku about artificial intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Operations\n",
    "\n",
    "Handle multiple requests concurrently for high-throughput applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_chat_completion(prompt: str, client: AsyncGroq) -> str:\n",
    "    \"\"\"Async chat completion for concurrent processing\"\"\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",  # Fastest model for high concurrency\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def process_multiple_prompts():\n",
    "    \"\"\"Process multiple prompts concurrently\"\"\"\n",
    "    async_client = AsyncGroq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    \n",
    "    prompts = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Explain neural networks\",\n",
    "        \"What is deep learning?\"\n",
    "    ]\n",
    "    \n",
    "    # Process all prompts concurrently\n",
    "    tasks = [async_chat_completion(prompt, async_client) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for prompt, result in zip(prompts, results):\n",
    "        print(f\"Q: {prompt}\")\n",
    "        print(f\"A: {result[:100]}...\\n\")\n",
    "\n",
    "# Run async example\n",
    "await process_multiple_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Use (Function Calling)\n",
    "\n",
    "Enable models to call external functions and APIs for dynamic capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Mock weather function - replace with real API\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 72°F\",\n",
    "        \"London\": \"Rainy, 60°F\",\n",
    "        \"Tokyo\": \"Cloudy, 68°F\"\n",
    "    }\n",
    "    return weather_data.get(location, \"Weather data not available\")\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Safe calculator function\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})\n",
    "\n",
    "def tool_use_example(user_query: str):\n",
    "    \"\"\"Demonstrate tool use with function calling\"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current weather for a location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Perform mathematical calculations\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Mathematical expression\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    \n",
    "    if response_message.tool_calls:\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        available_functions = {\n",
    "            \"get_weather\": get_weather,\n",
    "            \"calculate\": calculate\n",
    "        }\n",
    "        \n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = available_functions[function_name](**function_args)\n",
    "            \n",
    "            messages.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response\n",
    "            })\n",
    "        \n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return final_response.choices[0].message.content\n",
    "    \n",
    "    return response_message.content\n",
    "\n",
    "# Example usage\n",
    "result = tool_use_example(\"What's the weather in Tokyo and calculate 25 * 8?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### JSON Mode for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_output_example():\n",
    "    \"\"\"Generate structured JSON output\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a data extraction assistant. Return only valid JSON.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract key information about Python: {\\\"name\\\": \\\"\\\", \\\"type\\\": \\\"\\\", \\\"features\\\": [], \\\"year_created\\\": 0}\"\n",
    "            }\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "# Example usage\n",
    "structured_data = structured_output_example()\n",
    "print(json.dumps(structured_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "\n",
    "def robust_chat_completion(prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Chat completion with error handling and retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1024,\n",
    "                timeout=30\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except groq.APIConnectionError as e:\n",
    "            print(f\"Connection error (attempt {attempt + 1}): {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return \"Error: Could not connect to Groq API\"\n",
    "                \n",
    "        except groq.RateLimitError as e:\n",
    "            print(f\"Rate limit exceeded (attempt {attempt + 1}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                return \"Error: Rate limit exceeded\"\n",
    "                \n",
    "        except groq.APIStatusError as e:\n",
    "            print(f\"API error (attempt {attempt + 1}): {e.status_code} - {e.response}\")\n",
    "            return f\"Error: API returned status {e.status_code}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error (attempt {attempt + 1}): {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return \"Error: Unexpected error occurred\"\n",
    "\n",
    "# Example usage\n",
    "result = robust_chat_completion(\"Explain the benefits of Groq's LPU architecture\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound AI Systems\n",
    "\n",
    "Groq's latest compound-beta models integrate multiple capabilities including web search and code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_ai_example(query: str):\n",
    "    \"\"\"Use compound-beta for research with built-in tools\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"compound-beta\",  # AI system with built-in tools\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a research assistant with access to web search and code execution.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=2000,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage - this will use web search automatically\n",
    "research_result = compound_ai_example(\"What are the latest developments in AI inference acceleration?\")\n",
    "print(research_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips and Best Practices\n",
    "\n",
    "### Model Selection Guide:\n",
    "- **llama-3.1-8b-instant**: Fastest, best for simple tasks\n",
    "- **llama-3.3-70b-versatile**: Best balance of speed and quality\n",
    "- **compound-beta**: For research and complex reasoning\n",
    "\n",
    "### Optimization Strategies:\n",
    "1. **Use streaming** for better user experience\n",
    "2. **Implement async** for concurrent requests\n",
    "3. **Choose appropriate models** based on task complexity\n",
    "4. **Handle errors gracefully** with retry logic\n",
    "5. **Leverage tool use** for dynamic capabilities\n",
    "\n",
    "### Rate Limits:\n",
    "- Free tier: Generous limits for experimentation\n",
    "- Paid tiers: Higher throughput for production use\n",
    "- Monitor usage via the Groq console\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Groq's LPU architecture revolutionizes AI inference with unprecedented speed while maintaining high quality. The combination of fast inference, comprehensive tool support, and OpenAI compatibility makes Groq ideal for building responsive, intelligent applications.\n",
    "\n",
    "Start building with Groq today at [console.groq.com](https://console.groq.com)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
