{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM Tutorial: Unified API for 100+ LLMs\n",
    "\n",
    "LiteLLM is a Python library that provides a **unified interface** for calling 100+ Language Model APIs using the OpenAI format. It simplifies integration across providers like OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more.\n",
    "\n",
    "## Key Features\n",
    "- **Unified API**: Same interface for all providers\n",
    "- **Load Balancing**: Router with retry/fallback logic\n",
    "- **Cost Tracking**: Built-in spend monitoring\n",
    "- **Streaming Support**: Real-time response streaming\n",
    "- **Error Handling**: Consistent exception handling\n",
    "- **Async Support**: Full async/await compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LiteLLM\n",
    "!pip install litellm\n",
    "\n",
    "# Import required modules\n",
    "import litellm\n",
    "from litellm import completion, Router\n",
    "import os\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Completion Calls\n",
    "\n",
    "LiteLLM uses the **same `completion()` function** for all providers. Just change the model name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys (replace with your actual keys)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
    "\n",
    "# OpenAI GPT-4\n",
    "response = completion(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    ")\n",
    "print(\"OpenAI:\", response.choices[0].message.content)\n",
    "\n",
    "# Anthropic Claude\n",
    "response = completion(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    ")\n",
    "print(\"Claude:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Provider Examples\n",
    "\n",
    "LiteLLM supports **100+ providers** with consistent formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI\n",
    "os.environ[\"AZURE_API_KEY\"] = \"your-azure-key\"\n",
    "os.environ[\"AZURE_API_BASE\"] = \"your-azure-endpoint\"\n",
    "os.environ[\"AZURE_API_VERSION\"] = \"2023-05-15\"\n",
    "\n",
    "azure_response = completion(\n",
    "    model=\"azure/your-deployment-name\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}]\n",
    ")\n",
    "\n",
    "# Google Vertex AI\n",
    "os.environ[\"VERTEX_PROJECT\"] = \"your-project-id\"\n",
    "os.environ[\"VERTEX_LOCATION\"] = \"us-central1\"\n",
    "\n",
    "vertex_response = completion(\n",
    "    model=\"gemini-pro\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}]\n",
    ")\n",
    "\n",
    "# Local Ollama\n",
    "ollama_response = completion(\n",
    "    model=\"ollama/llama2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}],\n",
    "    api_base=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Support\n",
    "\n",
    "Get **real-time streaming responses** by setting `stream=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming response\n",
    "response = completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a short poem about AI\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Router: Load Balancing & Fallbacks\n",
    "\n",
    "The **Router** enables load balancing, retries, and fallbacks across multiple deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure multiple model deployments\n",
    "model_list = [\n",
    "    {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": \"azure/gpt-35-turbo\",\n",
    "            \"api_key\": os.getenv(\"AZURE_API_KEY\"),\n",
    "            \"api_base\": os.getenv(\"AZURE_API_BASE\"),\n",
    "            \"rpm\": 100  # requests per minute\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "            \"rpm\": 200\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create router with fallbacks\n",
    "router = Router(\n",
    "    model_list=model_list,\n",
    "    num_retries=3,\n",
    "    timeout=30,\n",
    "    fallbacks=[{\"gpt-3.5-turbo\": [\"gpt-4\"]}]  # fallback to GPT-4 if needed\n",
    ")\n",
    "\n",
    "# Make request through router\n",
    "response = router.completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello from router!\"}]\n",
    ")\n",
    "print(\"Router response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost Tracking & Callbacks\n",
    "\n",
    "LiteLLM provides **built-in cost tracking** and supports custom callbacks for monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to track costs\n",
    "def track_cost_callback(kwargs, completion_response, start_time, end_time):\n",
    "    \"\"\"Custom callback to log costs and usage\"\"\"\n",
    "    try:\n",
    "        response_cost = kwargs.get(\"response_cost\", 0)\n",
    "        model = kwargs.get(\"model\", \"unknown\")\n",
    "        print(f\"Model: {model}, Cost: ${response_cost:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in callback: {e}\")\n",
    "\n",
    "# Set callback\n",
    "litellm.success_callback = [track_cost_callback]\n",
    "\n",
    "# Test cost tracking\n",
    "response = completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Calculate 2+2\"}]\n",
    ")\n",
    "\n",
    "# Built-in logging to external services\n",
    "# litellm.success_callback = [\"langfuse\", \"helicone\", \"lunary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling & Retries\n",
    "\n",
    "LiteLLM standardizes **error handling** across all providers and supports automatic retries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAIError\n",
    "\n",
    "try:\n",
    "    # This will retry 3 times on failure\n",
    "    response = completion(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Test message\"}],\n",
    "        num_retries=3\n",
    "    )\n",
    "    print(\"Success:\", response.choices[0].message.content)\n",
    "    \n",
    "except OpenAIError as e:\n",
    "    print(f\"LiteLLM Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"General Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Async Support\n",
    "\n",
    "LiteLLM supports **async/await** for concurrent operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from litellm import acompletion\n",
    "\n",
    "async def async_completion_example():\n",
    "    \"\"\"Example of async completion\"\"\"\n",
    "    response = await acompletion(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello async world!\"}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def multiple_async_calls():\n",
    "    \"\"\"Make multiple concurrent API calls\"\"\"\n",
    "    tasks = [\n",
    "        acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"What is {topic}?\"}]\n",
    "        )\n",
    "        for topic in [\"AI\", \"Machine Learning\", \"Deep Learning\"]\n",
    "    ]\n",
    "    \n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"Response {i+1}: {response.choices[0].message.content[:50]}...\")\n",
    "\n",
    "# Run async examples\n",
    "# asyncio.run(multiple_async_calls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Configuration\n",
    "\n",
    "Additional features for production use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global configuration\n",
    "litellm.set_verbose = True  # Enable debug logging\n",
    "litellm.max_budget = 100.0  # Set spending limit ($100)\n",
    "\n",
    "# Context window fallbacks\n",
    "response = completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Long message...\"}],\n",
    "    fallbacks=[\"gpt-4-32k\"]  # Use model with larger context window\n",
    ")\n",
    "\n",
    "# Custom metadata\n",
    "response = completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    metadata={\n",
    "        \"user_id\": \"user123\",\n",
    "        \"session_id\": \"session456\",\n",
    "        \"custom_tag\": \"tutorial\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Advanced configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**LiteLLM** simplifies LLM integration by providing:\n",
    "\n",
    "1. **Unified API** - Same interface for 100+ providers\n",
    "2. **Reliability** - Built-in retries, fallbacks, and load balancing\n",
    "3. **Observability** - Cost tracking, logging, and monitoring\n",
    "4. **Performance** - Streaming and async support\n",
    "5. **Production-ready** - Error handling and advanced routing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
