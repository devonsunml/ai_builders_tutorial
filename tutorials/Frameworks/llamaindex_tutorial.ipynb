{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaIndex Tutorial: Building RAG Applications\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**LlamaIndex** is a powerful data framework that connects Large Language Models (LLMs) with your private data through **Retrieval-Augmented Generation (RAG)**. This tutorial covers the essential concepts for building intelligent applications that can query your documents.\n",
        "\n",
        "### What is RAG?\n",
        "RAG solves the problem that LLMs aren't trained on your specific data. It works by:\n",
        "1. **Loading** your documents \n",
        "2. **Indexing** them into searchable vectors\n",
        "3. **Retrieving** relevant chunks for user queries\n",
        "4. **Generating** responses using retrieved context\n",
        "\n",
        "### Key Components\n",
        "- **Documents**: Your data (PDFs, text files, web pages)\n",
        "- **Nodes**: Chunks of documents for processing\n",
        "- **Index**: Searchable structure (usually vector-based)\n",
        "- **Query Engine**: Interface for asking questions\n",
        "- **Embeddings**: Numerical representations of text meaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install required packages\n",
        "# !pip install llama-index openai python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    Document,\n",
        "    Settings\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI API key loaded from .env file\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Load API keys from .env file\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Verify API key is loaded\n",
        "if openai_api_key:\n",
        "    print(\"✅ OpenAI API key loaded from .env file\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "else:\n",
        "    print(\"❌ OpenAI API key not found in .env file\")\n",
        "    print(\"Please add OPENAI_API_KEY=your-actual-key to your .env file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure global settings\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 documents\n",
            "First document preview: \n",
            "LlamaIndex is a data framework for building LLM applications. \n",
            "It provides tools to ingest, structu...\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Create sample documents\n",
        "sample_text = \"\"\"\n",
        "LlamaIndex is a data framework for building LLM applications. \n",
        "It provides tools to ingest, structure, and access private data for LLMs.\n",
        "The framework supports various data sources including PDFs, databases, and APIs.\n",
        "RAG is the core technique that allows LLMs to answer questions about your data.\n",
        "\"\"\"\n",
        "\n",
        "documents = [Document(text=sample_text)]\n",
        "\n",
        "# Method 2: Load from directory (uncomment to use)\n",
        "# !mkdir -p data\n",
        "# documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "print(f\"First document preview: {documents[0].text[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Document Chunking (Node Parsing)\n",
        "\n",
        "**Chunking** splits documents into smaller pieces for better retrieval. Key considerations:\n",
        "- **Chunk Size**: Smaller chunks = more precise, larger chunks = more context\n",
        "- **Overlap**: Prevents information loss at boundaries\n",
        "- **Default**: 1024 tokens with 20 token overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 1 nodes\n",
            "First node: LlamaIndex is a data framework for building LLM applications. \n",
            "It provides tools to ingest, structure, and access private data for LLMs.\n",
            "The framework supports various data sources including PDFs, databases, and APIs.\n",
            "RAG is the core technique that allows LLMs to answer questions about your data.\n",
            "Node metadata: {}\n"
          ]
        }
      ],
      "source": [
        "# Configure text splitter\n",
        "text_splitter = SentenceSplitter(\n",
        "    chunk_size=512,  # Smaller chunks for demo\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# Parse documents into nodes\n",
        "nodes = text_splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "print(f\"Created {len(nodes)} nodes\")\n",
        "print(f\"First node: {nodes[0].text}\")\n",
        "print(f\"Node metadata: {nodes[0].metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Creating Vector Index\n",
        "\n",
        "**VectorStoreIndex** converts text into embeddings (numerical representations) for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5853dfc36d1461cb7e80c0b0f7edf5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dce42d08ea1e44e6b3811e7a83475465",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector index created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Direct from documents\n",
        "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
        "\n",
        "# Method 2: From nodes (more control)\n",
        "# index = VectorStoreIndex(nodes, show_progress=True)\n",
        "\n",
        "print(\"Vector index created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Building Query Engine\n",
        "\n",
        "**Query Engine** handles the retrieval and response generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: LlamaIndex is a data framework for building LLM applications that provides tools for ingesting, structuring, and accessing private data for LLMs.\n",
            "\n",
            "Source nodes:\n",
            "- Score: 0.894\n",
            "  Text: LlamaIndex is a data framework for building LLM applications. \n",
            "It provides tools to ingest, structur...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create query engine\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=2,  # Retrieve top 2 most similar chunks\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "# Test query\n",
        "response = query_engine.query(\"What is LlamaIndex?\")\n",
        "print(\"Answer:\", response)\n",
        "print(\"\\nSource nodes:\")\n",
        "for node in response.source_nodes:\n",
        "    print(f\"- Score: {node.score:.3f}\")\n",
        "    print(f\"  Text: {node.text[:100]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Advanced: Custom Retrieval\n",
        "\n",
        "Fine-tune retrieval for better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 1 nodes:\n",
            "\n",
            "Node 1 (Score: 0.794):\n",
            "LlamaIndex is a data framework for building LLM applications. \n",
            "It provides tools to ingest, structure, and access private data for LLMs.\n",
            "The framework...\n"
          ]
        }
      ],
      "source": [
        "# Get retriever for more control\n",
        "retriever = index.as_retriever(\n",
        "    similarity_top_k=3,\n",
        "    # filters=MetadataFilters(...) # Add metadata filters if needed\n",
        ")\n",
        "\n",
        "# Test retrieval\n",
        "retrieved_nodes = retriever.retrieve(\"How does RAG work?\")\n",
        "print(f\"Retrieved {len(retrieved_nodes)} nodes:\")\n",
        "for i, node in enumerate(retrieved_nodes):\n",
        "    print(f\"\\nNode {i+1} (Score: {node.score:.3f}):\")\n",
        "    print(node.text[:150] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Persistence and Storage\n",
        "\n",
        "Save your index to avoid reprocessing documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index saved to ./storage\n"
          ]
        }
      ],
      "source": [
        "# Save index\n",
        "index.storage_context.persist(persist_dir=\"./storage\")\n",
        "print(\"Index saved to ./storage\")\n",
        "\n",
        "# Load index (for future sessions)\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "# loaded_index = load_index_from_storage(storage_context)\n",
        "# query_engine = loaded_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Interactive Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is LlamaIndex?\n",
            "Answer: LlamaIndex is a data framework designed for creating LLM applications, offering tools for managing private data from different sources like PDFs, databases, and APIs. It utilizes the RAG technique to enable LLMs to analyze and respond to inquiries about the data.\n",
            "--------------------------------------------------\n",
            "Question: How does RAG work?\n",
            "Answer: RAG works as the core technique that enables LLMs to respond to inquiries regarding the data by utilizing the tools provided within the LlamaIndex framework.\n",
            "--------------------------------------------------\n",
            "Question: What data sources does LlamaIndex support?\n",
            "Answer: LlamaIndex supports various data sources including PDFs, databases, and APIs.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Interactive query function\n",
        "def ask_question(question):\n",
        "    response = query_engine.query(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {response}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Test different questions\n",
        "questions = [\n",
        "    \"What is LlamaIndex?\",\n",
        "    \"How does RAG work?\",\n",
        "    \"What data sources does LlamaIndex support?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    ask_question(q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Best Practices\n",
        "1. **Chunk Size**: Start with 1024 tokens, adjust based on your data\n",
        "2. **Embeddings**: Choose models suited to your domain\n",
        "3. **Retrieval**: Experiment with `similarity_top_k` values\n",
        "4. **Persistence**: Always save indexes for production use\n",
        "\n",
        "### Next Steps\n",
        "- Explore different **node parsers** (Semantic, Hierarchical)\n",
        "- Try **hybrid search** combining semantic and keyword search  \n",
        "- Implement **metadata filtering** for precise retrieval\n",
        "- Build **chat engines** for conversational interfaces\n",
        "- Use **agents** for multi-step reasoning\n",
        "\n",
        "### Performance Tips\n",
        "- Use **smaller chunks** for precise answers\n",
        "- Use **larger chunks** for comprehensive context\n",
        "- Consider **vector databases** (Pinecone, Chroma) for scaling\n",
        "- Implement **evaluation metrics** to measure quality\n",
        "\n",
        "LlamaIndex makes building RAG applications straightforward while providing flexibility for advanced use cases. Start simple and gradually add complexity as needed!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.10.14",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
