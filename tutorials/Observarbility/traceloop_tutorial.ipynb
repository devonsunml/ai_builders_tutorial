{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Traceloop Tutorial: Complete Guide to LLM Observability\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Traceloop is an open-source LLM observability platform that monitors what your model says, how fast it responds, and when things start to slip ‚Äî so you can debug faster and deploy safely. It provides real-time alerts about your model's quality, execution tracing for every request, and helps you gradually rollout changes to models and prompts.\n",
        "\n",
        "### What is OpenLLMetry?\n",
        "\n",
        "OpenLLMetry is a set of extensions built on top of OpenTelemetry that gives you complete observability over your LLM application. It's non-intrusive and can be connected to your existing observability solutions like Datadog, Honeycomb, and others.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **One-line setup**: Get instant monitoring with minimal code changes\n",
        "- **Multi-provider support**: Supports 20+ providers (OpenAI, Anthropic, Gemini, Bedrock, Ollama), vector DBs (Pinecone, Chroma), and frameworks like LangChain, LlamaIndex, and CrewAI\n",
        "- **Quality evaluation**: Built-in metrics for faithfulness, relevance, and safety\n",
        "- **Custom evaluators**: Define what quality means for your specific use case\n",
        "- **OpenTelemetry compatibility**: Integrates with existing observability stacks\n",
        "\n",
        "## Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install traceloop-sdk openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from traceloop.sdk import Traceloop\n",
        "from traceloop.sdk.decorators import workflow\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set your API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
        "# Optional: Set Traceloop API key for cloud dashboard\n",
        "# os.environ[\"TRACELOOP_API_KEY\"] = \"your-traceloop-api-key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Setup and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Traceloop - this enables automatic tracing\n",
        "Traceloop.init(\n",
        "    app_name=\"traceloop_tutorial\",\n",
        "    disable_batch=True  # For immediate trace visibility in notebooks\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Traceloop initialized successfully!\")\n",
        "print(\"üìä Dashboard will be available after running LLM calls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 1: Basic LLM Tracing\n",
        "\n",
        "Traceloop automatically instruments popular LLM providers. No additional code changes needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create OpenAI client - this will be automatically instrumented\n",
        "client = OpenAI()\n",
        "\n",
        "# Simple LLM call - automatically traced\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain LLM observability in one sentence.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(\"Response:\", response.choices[0].message.content)\n",
        "print(\"\\nüîç This call was automatically traced by Traceloop!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 2: Custom Workflows with Decorators\n",
        "\n",
        "Use `@workflow` decorator to trace complex functions and get better insights into your application logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@workflow(name=\"story_generator\")\n",
        "def generate_story(theme, length=\"short\"):\n",
        "    \"\"\"Generate a story with custom workflow tracing\"\"\"\n",
        "    \n",
        "    # This entire function will be traced as a single workflow\n",
        "    prompt = f\"Write a {length} story about {theme}. Make it engaging and creative.\"\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.9,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test the workflow\n",
        "story = generate_story(\"artificial intelligence\", \"medium\")\n",
        "print(\"Generated Story:\")\n",
        "print(story)\n",
        "print(\"\\nüìà This workflow is now traceable in your dashboard!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 3: Multi-Step Workflows\n",
        "\n",
        "Track complex pipelines with multiple LLM calls and processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@workflow(name=\"content_analysis_pipeline\")\n",
        "def analyze_content(text):\n",
        "    \"\"\"Multi-step content analysis pipeline\"\"\"\n",
        "    \n",
        "    # Step 1: Sentiment Analysis\n",
        "    sentiment_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Analyze sentiment. Respond with: Positive, Negative, or Neutral.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
        "        ],\n",
        "        max_tokens=10\n",
        "    )\n",
        "    sentiment = sentiment_response.choices[0].message.content.strip()\n",
        "    \n",
        "    # Step 2: Key Topics Extraction\n",
        "    topics_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Extract 3 main topics. Return as comma-separated list.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
        "        ],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    topics = topics_response.choices[0].message.content.strip()\n",
        "    \n",
        "    # Step 3: Summary Generation\n",
        "    summary_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Create a brief summary in 2-3 sentences.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    summary = summary_response.choices[0].message.content.strip()\n",
        "    \n",
        "    return {\n",
        "        \"sentiment\": sentiment,\n",
        "        \"topics\": topics,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "# Test the pipeline\n",
        "sample_text = \"Artificial intelligence is revolutionizing healthcare by enabling faster diagnosis and personalized treatment plans. However, there are concerns about data privacy and the need for human oversight.\"\n",
        "\n",
        "analysis = analyze_content(sample_text)\n",
        "print(\"Content Analysis Results:\")\n",
        "print(f\"Sentiment: {analysis['sentiment']}\")\n",
        "print(f\"Topics: {analysis['topics']}\")\n",
        "print(f\"Summary: {analysis['summary']}\")\n",
        "print(\"\\nüîó All steps are traced as a connected workflow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 4: Framework Integration (LangChain Example)\n",
        "\n",
        "Traceloop automatically instruments popular frameworks like LangChain without additional configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: This is a conceptual example. Install langchain if you want to run it:\n",
        "# !pip install langchain langchain-openai\n",
        "\n",
        "# Uncomment below to test LangChain integration:\n",
        "\"\"\"\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# LangChain LLM - automatically instrumented by Traceloop\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Create messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
        "    HumanMessage(content=\"Explain the benefits of using observability in ML applications.\")\n",
        "]\n",
        "\n",
        "# This call will be automatically traced\n",
        "response = llm(messages)\n",
        "print(\"LangChain Response:\", response.content)\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìö LangChain integration works automatically with Traceloop!\")\n",
        "print(\"üîß Just initialize Traceloop and use LangChain normally.\")\n",
        "print(\"üí° Uncomment the code above to test LangChain tracing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 5: Configuration and Advanced Features\n",
        "\n",
        "Configure Traceloop for different environments and use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced configuration example\n",
        "def setup_production_tracing():\n",
        "    \"\"\"Example of production-ready Traceloop configuration\"\"\"\n",
        "    \n",
        "    # Configuration for sending to external observability platform\n",
        "    config = {\n",
        "        \"app_name\": \"production_llm_app\",\n",
        "        \"api_endpoint\": \"https://your-otel-collector.com\",  # Your OTEL endpoint\n",
        "        \"headers\": {\n",
        "            \"Authorization\": \"Bearer your-token\",\n",
        "            \"X-Custom-Header\": \"production\"\n",
        "        },\n",
        "        \"disable_batch\": False,  # Enable batching for production\n",
        "        \"resource_attributes\": {\n",
        "            \"service.name\": \"llm-service\",\n",
        "            \"service.version\": \"1.0.0\",\n",
        "            \"environment\": \"production\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return config\n",
        "\n",
        "# Environment-specific configuration\n",
        "def get_traceloop_config(environment=\"development\"):\n",
        "    \"\"\"Get environment-specific configuration\"\"\"\n",
        "    \n",
        "    if environment == \"production\":\n",
        "        return setup_production_tracing()\n",
        "    elif environment == \"staging\":\n",
        "        return {\n",
        "            \"app_name\": \"staging_llm_app\",\n",
        "            \"disable_batch\": False\n",
        "        }\n",
        "    else:  # development\n",
        "        return {\n",
        "            \"app_name\": \"dev_llm_app\",\n",
        "            \"disable_batch\": True  # See traces immediately\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "dev_config = get_traceloop_config(\"development\")\n",
        "print(\"Development Configuration:\")\n",
        "for key, value in dev_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  Configure Traceloop based on your deployment environment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 6: Monitoring Key Metrics\n",
        "\n",
        "Understanding what Traceloop tracks automatically and how to interpret the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@workflow(name=\"metrics_demo\")\n",
        "def demonstrate_metrics():\n",
        "    \"\"\"Demonstrate different metrics that Traceloop captures\"\"\"\n",
        "    \n",
        "    # Different types of calls to show various metrics\n",
        "    calls_data = []\n",
        "    \n",
        "    # Call 1: Short prompt, low temperature\n",
        "    response1 = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=10\n",
        "    )\n",
        "    calls_data.append({\"type\": \"short_precise\", \"tokens\": 10, \"temp\": 0.1})\n",
        "    \n",
        "    # Call 2: Longer prompt, higher temperature\n",
        "    response2 = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\n",
        "            \"role\": \"user\", \n",
        "            \"content\": \"Write a creative poem about machine learning and observability\"\n",
        "        }],\n",
        "        temperature=0.9,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    calls_data.append({\"type\": \"long_creative\", \"tokens\": 150, \"temp\": 0.9})\n",
        "    \n",
        "    # Call 3: System + User messages\n",
        "    response3 = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a technical expert.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain distributed tracing benefits.\"}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    calls_data.append({\"type\": \"system_user\", \"tokens\": 100, \"temp\": 0.5})\n",
        "    \n",
        "    return calls_data\n",
        "\n",
        "# Run the metrics demonstration\n",
        "metrics_data = demonstrate_metrics()\n",
        "\n",
        "print(\"üìä Metrics Being Tracked by Traceloop:\")\n",
        "print(\"\\nüîç Automatic Metrics:\")\n",
        "print(\"  ‚Ä¢ Latency: Response time for each call\")\n",
        "print(\"  ‚Ä¢ Token Usage: Input and output tokens\")\n",
        "print(\"  ‚Ä¢ Cost: Estimated cost per call\")\n",
        "print(\"  ‚Ä¢ Model Performance: Success/failure rates\")\n",
        "print(\"  ‚Ä¢ Prompt-Response Pairs: Complete conversation tracking\")\n",
        "\n",
        "print(\"\\nüìà Quality Metrics:\")\n",
        "print(\"  ‚Ä¢ Faithfulness: How well responses match input\")\n",
        "print(\"  ‚Ä¢ Relevance: How relevant responses are to prompts\")\n",
        "print(\"  ‚Ä¢ Safety: Detection of harmful content\")\n",
        "print(\"  ‚Ä¢ Custom Evaluators: Your domain-specific quality measures\")\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {len(metrics_data)} traced calls with different characteristics!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing Your Traces\n",
        "\n",
        "After running the above examples, you can view your traces in several ways:\n",
        "\n",
        "### Option 1: Traceloop Cloud Dashboard\n",
        "- Sign up at [traceloop.com](https://www.traceloop.com)\n",
        "- Get your API key and set `TRACELOOP_API_KEY`\n",
        "- Re-run `Traceloop.init()` with your API key\n",
        "\n",
        "### Option 2: Local Development Dashboard\n",
        "- Traceloop provides a temporary local dashboard URL when you run traces\n",
        "- Look for dashboard links in your console output\n",
        "\n",
        "### Option 3: Export to Existing Tools\n",
        "- Configure OpenTelemetry endpoint to send to Datadog, Honeycomb, etc.\n",
        "- Use the `api_endpoint` and `headers` parameters in `Traceloop.init()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices and Tips\n",
        "\n",
        "### 1. Production Deployment\n",
        "- Always use `disable_batch=False` in production for better performance\n",
        "- Set appropriate resource attributes for filtering and organization\n",
        "- Configure sampling rates for high-traffic applications\n",
        "\n",
        "### 2. Security Considerations\n",
        "- Traceloop captures prompts and responses - ensure compliance with data policies\n",
        "- Use environment variables for API keys\n",
        "- Consider data retention policies for sensitive information\n",
        "\n",
        "### 3. Workflow Organization\n",
        "- Use descriptive names for `@workflow` decorators\n",
        "- Group related LLM calls into logical workflows\n",
        "- Add custom attributes to spans for better filtering\n",
        "\n",
        "### 4. Cost Optimization\n",
        "- Monitor token usage patterns through traces\n",
        "- Use temperature and max_tokens strategically\n",
        "- Track model performance vs. cost trade-offs\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Traceloop provides enterprise-grade LLM observability with just one line of code, helping you monitor, debug, and improve your LLM applications. It's built on OpenTelemetry standards, ensuring compatibility with existing observability stacks while providing LLM-specific insights.\n",
        "\n",
        "**Next Steps:**\n",
        "- Explore the [Traceloop documentation](https://www.traceloop.com/docs) for advanced features\n",
        "- Try integrating with your existing frameworks (LangChain, LlamaIndex)\n",
        "- Set up custom evaluators for your specific use cases\n",
        "- Configure alerts and monitoring for production deployments\n",
        "\n",
        "**Resources:**\n",
        "- [OpenLLMetry GitHub](https://github.com/traceloop/openllmetry)\n",
        "- [Traceloop Community Slack](https://traceloop.com/slack)\n",
        "- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}