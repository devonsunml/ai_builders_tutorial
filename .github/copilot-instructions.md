# GitHub Copilot Instructions

## Token Limits for LLM Interactions

When interacting with LLM models in Copilot Chat:

- **Maximum input tokens**: 130,000
- **Maximum output tokens**: 130,000

Ensure all prompts and responses stay within these limits to prevent truncation, rate limiting, or other errors.

- in the prompt message, make sure input is no more than input token limit also specify the output token response message should be within output token limit to prevent errors.
