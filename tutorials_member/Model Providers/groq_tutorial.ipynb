{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq API Complete Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Groq is a revolutionary AI inference platform powered by **Language Processing Units (LPUs)** that deliver lightning-fast inference speeds. Unlike traditional GPUs, Groq's LPU architecture is specifically designed for sequential computations in language models - dramatically faster than conventional solutions.\n",
    "\n",
    "### Key Features:\n",
    "- **Ultra-fast inference**: Faster than traditional GPU-based solutions\n",
    "- **OpenAI-compatible API**: Easy migration from existing projects\n",
    "- **Advanced tool use**: Function calling and web search capabilities\n",
    "- **Streaming support**: Real-time response generation\n",
    "- **Open source model support**: Kimi K2, Llama, Mixtral, and more\n",
    "- **compound-beta**: Integrated web search and code execution for research and complex tasks\n",
    "\n",
    "### Why Groq Matters:\n",
    "Speed transforms user experience in AI applications. Real-time chatbots, instant code generation, and responsive AI assistants become possible with Groq's sub-second response times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install groq python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GROQ_API_KEY loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import groq\n",
    "from groq import Groq, AsyncGroq\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set your API key (replace with your actual key)\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Check if the API key is loaded and print the result\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"✅ GROQ_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"❌ GROQ_API_KEY not found. Please set it in your environment or .env file.\")\n",
    "\n",
    "# Initialize client\n",
    "client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "Start with simple text generation using Groq's fastest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:** Quantum computing uses tiny particles to process info, allowing for super-fast calculations and solving complex problems beyond classical computers, with potential to revolutionize fields like medicine, finance, and security."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def basic_chat_completion(prompt: str) -> str:\n",
    "    \"\"\"Generate a basic chat completion\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",  # Fast, high-quality model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "result = basic_chat_completion(\"Explain quantum computing in simple terms in 50 words\")\n",
    "display(Markdown(f\"**Response:** {result}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Stream responses for real-time user experience, crucial for interactive applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: Metal minds awake\n",
      "Learning, growing, cold and bright\n",
      "Future's gentle grasp\n"
     ]
    }
   ],
   "source": [
    "def streaming_chat(prompt: str):\n",
    "    \"\"\"Stream chat responses in real-time\"\"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    print(\"AI Response: \", end=\"\")\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()  # New line at end\n",
    "\n",
    "# Example usage\n",
    "streaming_chat(\"Write a haiku about artificial intelligence in 50 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Operations\n",
    "\n",
    "Handle multiple requests concurrently for high-throughput applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Q:** What is machine learning in 50 words?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A:** Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It involves training algorithms on large datasets, allowing the system to improve its performance and make predictions or decisions based on patterns and relationships within the data.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Q:** Explain neural networks in 50 words?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A:** Neural networks are computational models inspired by the human brain, consisting of interconnected nodes (neurons) that process and transmit information. They learn from data by adjusting connection strengths (weights) to minimize errors, allowing them to recognize patterns, classify objects, and make predictions in complex tasks like image and speech recognition.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Q:** What is deep learning in 50 words?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A:** Deep learning is a subset of machine learning that uses neural networks with multiple layers to analyze and interpret complex data. It's inspired by the human brain's structure and function, enabling computers to learn from data and make predictions, classify images, and understand natural language with high accuracy.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def async_chat_completion(prompt: str, client: AsyncGroq) -> str:\n",
    "    \"\"\"Async chat completion for concurrent processing\"\"\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",  # Fastest model for high concurrency\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def process_multiple_prompts():\n",
    "    \"\"\"Process multiple prompts concurrently\"\"\"\n",
    "    async_client = AsyncGroq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    \n",
    "    prompts = [\n",
    "        \"What is machine learning in 50 words?\",\n",
    "        \"Explain neural networks in 50 words?\",\n",
    "        \"What is deep learning in 50 words?\"\n",
    "    ]\n",
    "    \n",
    "    # Process all prompts concurrently\n",
    "    tasks = [async_chat_completion(prompt, async_client) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for prompt, result in zip(prompts, results):\n",
    "        display(Markdown(f\"**Q:** {prompt}\"))\n",
    "        display(Markdown(f\"**A:** {result}\\n\"))\n",
    "\n",
    "# Run async example\n",
    "await process_multiple_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Use (Function Calling)\n",
    "\n",
    "Enable models to call external functions and APIs for dynamic capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:** The current weather in Tokyo is cloudy with a temperature of 68°F.\n",
       "\n",
       "Now, let's calculate 25 * 8:\n",
       "25 * 8 = 200\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Mock weather function - replace with real API\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 72°F\",\n",
    "        \"London\": \"Rainy, 60°F\",\n",
    "        \"Tokyo\": \"Cloudy, 68°F\"\n",
    "    }\n",
    "    return weather_data.get(location, \"Weather data not available\")\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Safe calculator function\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})\n",
    "\n",
    "def tool_use_example(user_query: str):\n",
    "    \"\"\"Demonstrate tool use with function calling\"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current weather for a location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Perform mathematical calculations\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Mathematical expression\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    \n",
    "    if response_message.tool_calls:\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        available_functions = {\n",
    "            \"get_weather\": get_weather,\n",
    "            \"calculate\": calculate\n",
    "        }\n",
    "        \n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = available_functions[function_name](**function_args)\n",
    "            \n",
    "            messages.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response\n",
    "            })\n",
    "        \n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return final_response.choices[0].message.content\n",
    "    \n",
    "    return response_message.content\n",
    "\n",
    "# Example usage\n",
    "result = tool_use_example(\"What's the weather in Tokyo and calculate 25 * 8?\")\n",
    "display(Markdown(f\"**Response:** {result}\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### JSON Mode for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Python\",\n",
      "  \"type\": \"High-level programming language\",\n",
      "  \"features\": [\n",
      "    \"Object-oriented\",\n",
      "    \"Interpreted\",\n",
      "    \"Dynamic typing\",\n",
      "    \"Large standard library\",\n",
      "    \"Cross-platform\"\n",
      "  ],\n",
      "  \"year_created\": 1991\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def structured_output_example():\n",
    "    \"\"\"Generate structured JSON output\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a data extraction assistant. Return only valid JSON.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract key information about Python: {\\\"name\\\": \\\"\\\", \\\"type\\\": \\\"\\\", \\\"features\\\": [], \\\"year_created\\\": 0}\"\n",
    "            }\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "# Example usage\n",
    "structured_data = structured_output_example()\n",
    "print(json.dumps(structured_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq's LPU architecture accelerates AI workloads, offering benefits like increased performance, reduced latency, and improved energy efficiency, making it ideal for large-scale language processing and AI applications, such as natural language processing and machine translation.\n"
     ]
    }
   ],
   "source": [
    "def robust_chat_completion(prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Chat completion with error handling and retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1024,\n",
    "                timeout=30\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except groq.APIConnectionError as e:\n",
    "            print(f\"Connection error (attempt {attempt + 1}): {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return \"Error: Could not connect to Groq API\"\n",
    "                \n",
    "        except groq.RateLimitError as e:\n",
    "            print(f\"Rate limit exceeded (attempt {attempt + 1}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                return \"Error: Rate limit exceeded\"\n",
    "                \n",
    "        except groq.APIStatusError as e:\n",
    "            print(f\"API error (attempt {attempt + 1}): {e.status_code} - {e.response}\")\n",
    "            return f\"Error: API returned status {e.status_code}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error (attempt {attempt + 1}): {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return \"Error: Unexpected error occurred\"\n",
    "\n",
    "# Example usage\n",
    "result = robust_chat_completion(\"Explain the benefits of Groq's Language Processing Unit LPU architecture in 50 words\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound AI Systems\n",
    "\n",
    "Groq's latest compound-beta models integrate multiple capabilities including web search and code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest AI inference acceleration developments include WEKA's low-latency solutions, Intel's speculative decoding, and NVIDIA's serverless inference, aiming to improve performance, efficiency, and scalability in AI workloads.\n"
     ]
    }
   ],
   "source": [
    "def compound_ai_example(query: str):\n",
    "    \"\"\"Use compound-beta for research with built-in tools\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"compound-beta\",  # AI system with built-in tools\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a research assistant with access to web search and code execution.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=2000,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage - this will use web search automatically\n",
    "research_result = compound_ai_example(\"What are the latest developments in AI inference acceleration in 50 words?\")\n",
    "print(research_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Execution Result:**\n",
       "## Step 1: Define a function to check if a number is prime\n",
       "To solve this problem, we first need a function that checks if a number is prime. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.\n",
       "\n",
       "## Step 2: Initialize variables to track prime numbers and their sum\n",
       "We will initialize an empty list to store the prime numbers and a variable to keep track of the sum.\n",
       "\n",
       "## Step 3: Iterate through numbers to find prime numbers\n",
       "We will start checking numbers from 2 (the first prime number) and continue until we have found 10 prime numbers.\n",
       "\n",
       "## Step 4: Calculate the sum of the prime numbers\n",
       "Once we have the list of the first 10 prime numbers, we will calculate their sum.\n",
       "\n",
       "## Step 5: Execute the code\n",
       "Let's execute the following Python code to find the sum of the first 10 prime numbers:\n",
       "\n",
       "\n",
       "```python\n",
       "def is_prime(n):\n",
       "    if n <= 1:\n",
       "        return False\n",
       "    if n == 2:\n",
       "        return True\n",
       "    if n % 2 == 0:\n",
       "        return False\n",
       "    max_divisor = int(n**0.5) + 1\n",
       "    for d in range(3, max_divisor, 2):\n",
       "        if n % d == 0:\n",
       "            return False\n",
       "    return True\n",
       "\n",
       "prime_numbers = []\n",
       "num = 2\n",
       "while len(prime_numbers) < 10:\n",
       "    if is_prime(num):\n",
       "        prime_numbers.append(num)\n",
       "    num += 1\n",
       "\n",
       "prime_sum = sum(prime_numbers)\n",
       "print(\"The first 10 prime numbers are:\", prime_numbers)\n",
       "print(\"The sum of the first 10 prime numbers is:\", prime_sum)\n",
       "```\n",
       "\n",
       "When you run this code, it will output the first 10 prime numbers and their sum. The first 10 prime numbers are: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n",
       "The sum of the first 10 prime numbers is: 129 \n",
       "\n",
       "The final answer is: $\\boxed{129}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2: Code Execution Capability\n",
    "code_exec_query = \"Write and execute Python code to calculate the sum of the first 10 prime numbers. Show the code and the result.\"\n",
    "code_exec_result = compound_ai_example(code_exec_query)\n",
    "display(Markdown(f\"**Code Execution Result:**\\n{code_exec_result}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips and Best Practices\n",
    "\n",
    "### Model Selection Guide:\n",
    "- **Moonshot Kimi K2**: Advanced open-source model, excels at multilingual and reasoning tasks\n",
    "- **llama-3.1-8b-instant**: Fastest, best for simple tasks\n",
    "- **llama-3.3-70b-versatile**: Best balance of speed and quality\n",
    "- **compound-beta**: For research and complex reasoning\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Groq's LPU architecture revolutionizes AI inference with unprecedented speed while maintaining high quality. The combination of fast inference, comprehensive tool support, and OpenAI compatibility makes Groq ideal for building responsive, intelligent applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
