{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit for LLM Chatbots: A Beginner's Tutorial\n",
    "\n",
    "This tutorial will guide you through building and running a conversational AI application using **Streamlit** and a Large Language Model (LLM), all from within a Jupyter/Colab notebook. We will cover the essential concepts to get you started. üí¨\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "* **Chat UI with Streamlit**: Use `st.chat_message` and `st.chat_input` to create a user-friendly chat interface.\n",
    "* **Session State for Memory**: Understand how `st.session_state` maintains conversation history.\n",
    "* **Streaming Responses**: Implement a real-time \"typing\" effect for LLM responses, as demonstrated in the official Streamlit documentation.\n",
    "* **How to use ngrok to run your Streamlit app from a notebook**: Learn to expose your local Streamlit app to the internet using ngrok, making it accessible from anywhere.\n",
    "* **Running Streamlit in a Notebook**: Learn how to use `pyngrok` to serve your Streamlit app from a notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites and Setup\n",
    "\n",
    "First, you'll need to install the necessary Python libraries. `pyngrok` is essential for exposing the Streamlit service running inside the notebook to a public URL that you can access. Run the following command in your terminal or a code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install the required packages\n",
    "# !pip install streamlit openai python-dotenv pyngrok -q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a file named `.env` in your project's root directory. This file will store your OpenAI API key securely. Add the following line, replacing `your_api_key_here` with your actual OpenAI API key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Streamlit App File\n",
    "\n",
    "Instead of creating a separate file manually, we can use a Jupyter magic command `%%writefile` to create our `app.py`. This command saves the content of the cell into a file named `app.py` in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "st.title(\"ü§ñ Simple LLM Chat App\")\n",
    "\n",
    "# Initialize chat history in session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Accept user input\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    # Add user message to history and display it\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generate and stream assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "        # The streaming feature is enabled here\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
    "                for m in st.session_state.messages\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "        # Process the stream and display the response in real-time\n",
    "        for chunk in stream:\n",
    "            full_response += (chunk.choices[0].delta.content or \"\")\n",
    "            message_placeholder.markdown(full_response + \"‚ñå\")\n",
    "        message_placeholder.markdown(full_response)\n",
    "    # Add the final assistant response to history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the App from the Notebook\n",
    "\n",
    "Now we'll run the app. As explained in this [helpful guide](https://deveshsurve.medium.com/how-to-serve-an-llm-as-a-streamlit-app-on-google-colab-step-by-step-guide-b6d9dc45427d), we will launch the Streamlit app on a local port and then use `pyngrok` to create a secure tunnel, giving us a public URL to access our app. \n",
    "\n",
    "Execute the cell below. It will start the Streamlit server in the background and print a public Ngrok URL. Click on the URL to open your chatbot in a new tab! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Streamlit to launch...\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://192.168.68.63:8501\n",
      "\n",
      "  For better performance, install the Watchdog module:\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://192.168.68.63:8501\n",
      "\n",
      "  For better performance, install the Watchdog module:\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2025-08-05T20:46:09+0800 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=\"/Users/devon/Library/Application Support/ngrok/ngrok.yml\" legacy_path=/Users/devon/.ngrok2/ngrok.yml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit App URL: NgrokTunnel: \"https://5894d923eeab.ngrok-free.app\" -> \"http://localhost:8501\"\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok, conf\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set your ngrok authtoken (replace 'your-ngrok-authtoken' with your actual token)\n",
    "NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\") or \"your-ngrok-authtoken\"\n",
    "if NGROK_AUTH_TOKEN == \"your-ngrok-authtoken\":\n",
    "    print(\"‚ùó Please set your ngrok authtoken in the NGROK_AUTH_TOKEN environment variable or replace 'your-ngrok-authtoken' in the code.\")\n",
    "else:\n",
    "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
    "\n",
    "    # Terminate any existing tunnels\n",
    "    ngrok.kill()\n",
    "\n",
    "    # Start the Streamlit app in the background using subprocess\n",
    "    process = subprocess.Popen([\n",
    "        \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"\n",
    "    ])\n",
    "\n",
    "    # Wait a few seconds to ensure Streamlit starts\n",
    "    print(\"Waiting for Streamlit to launch...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Open a tunnel to the Streamlit port\n",
    "    public_url = ngrok.connect(8501)\n",
    "    print(f\"Streamlit App URL: {public_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code Breakdown\n",
    "\n",
    "### Streaming for a Better UX\n",
    "This app includes a crucial feature for conversational AI: **response streaming**. By setting `stream=True` in the OpenAI API call, we receive the LLM's response word-by-word instead of waiting for the entire message. \n",
    "\n",
    "The code then iterates through each `chunk` of the response, appends it to `full_response`, and immediately updates the placeholder on the screen (`message_placeholder.markdown(full_response + \"‚ñå\")`). This creates the familiar \"typing\" animation, making the app feel much more responsive and interactive.\n",
    "\n",
    "### Session State for Memory\n",
    "Streamlit reruns the script on each interaction. To remember the conversation, `st.session_state.messages` acts as our chat's memory. We initialize it once and append new user and assistant messages to it, ensuring the LLM has the full context of the conversation for its next response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "You have now successfully built and launched a streaming LLM chatbot directly from a Jupyter notebook. This workflow is excellent for rapid prototyping and testing. From here, you can continue to build out more complex features or prepare your app for deployment. Happy coding! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
